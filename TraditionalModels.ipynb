{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYi8Uz-IPYF8",
    "outputId": "a7f3ac58-afc8-4b05-9cc8-6871d92d3625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn transformers nltk scikit-learn xgboost tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, GlobalMaxPooling1D, Conv1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')  # Ensure WordNet for lemmatization is downloaded\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu8bFcj4PYDm",
    "outputId": "1cd1b1a4-9b9b-46d4-9a7a-e915edbcfe94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Post ID                                   Post description        Date  \\\n",
      "0  CgXDOaQDvGm  “I have decided that the global #monkeypox out...  07/23/2022   \n",
      "1  CgXpRmMIdzG  In light of the evolving monkeypox outbreak wi...  07/23/2022   \n",
      "2  CgXaFGDsevq  If you've been hearing about monkeypox and wan...  07/23/2022   \n",
      "3  CgXGNrmLwoL  Monkeypox is a rare disease caused by infectio...  07/23/2022   \n",
      "4  CgXTqcjOQD-  For today's @newyorkermag dispatch. \\n'The Ago...  07/23/2022   \n",
      "\n",
      "  Language                        Translated Post Description Sentiment  \\\n",
      "0  English  “I have decided that the global #monkeypox out...   neutral   \n",
      "1  English  In light of the evolving monkeypox outbreak wi...   neutral   \n",
      "2  English  If you've been hearing about monkeypox and wan...   neutral   \n",
      "3  English  Monkeypox is a rare disease caused by infectio...   neutral   \n",
      "4  English  For today's @newyorkermag dispatch. \\n'The Ago...   sadness   \n",
      "\n",
      "       Hate           Stress or Anxiety  \n",
      "0  Not Hate     Stress/Anxiety Detected  \n",
      "1  Not Hate     Stress/Anxiety Detected  \n",
      "2  Not Hate  No Stress/Anxiety Detected  \n",
      "3  Not Hate  No Stress/Anxiety Detected  \n",
      "4  Not Hate  No Stress/Anxiety Detected  \n",
      "Sentiment\n",
      "neutral    6000\n",
      "joy        5223\n",
      "sadness    3572\n",
      "anger      1617\n",
      "Name: count, dtype: int64\n",
      "['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/ProjectMonkeyPox/Monkeypox Dataset.csv')\n",
    "\n",
    "# Check dataset structure\n",
    "print(df.head())\n",
    "\n",
    "# Original sentiment distribution\n",
    "print(df['Sentiment'].value_counts())\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "    text = re.sub(r\"#\", \"\", text)  # Remove hashtags\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)  # Remove special characters and numbers\n",
    "    return text.lower()\n",
    "\n",
    "df[\"Cleaned_Text\"] = df[\"Translated Post Description\"].apply(clean_text)\n",
    "\n",
    "# Preprocess text with lemmatization and stopwords removal\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"Processed_Text\"] = df[\"Cleaned_Text\"].apply(preprocess_text)\n",
    "\n",
    "# Sentiment mapping\n",
    "sentiment_mapping = {\n",
    "    \"anger\": \"Negative\",\n",
    "    \"sadness\": \"Negative\",\n",
    "    \"neutral\": \"Neutral\",\n",
    "    \"joy\": \"Positive\"\n",
    "}\n",
    "\n",
    "df[\"Merged_Sentiment\"] = df[\"Sentiment\"].map(sentiment_mapping)\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Sentiment_Encoded\"] = label_encoder.fit_transform(df[\"Merged_Sentiment\"])\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"Processed_Text\"], df[\"Sentiment_Encoded\"], test_size=0.2, stratify=df[\"Sentiment_Encoded\"], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEZCFfnVPYA8",
    "outputId": "e2bd08ef-b33d-45dc-abef-86890076e09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75      1038\n",
      "           1       0.80      0.83      0.82      1200\n",
      "           2       0.83      0.80      0.81      1045\n",
      "\n",
      "    accuracy                           0.80      3283\n",
      "   macro avg       0.80      0.79      0.79      3283\n",
      "weighted avg       0.80      0.80      0.80      3283\n",
      "\n",
      "Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.33      0.47      1038\n",
      "           1       0.54      0.95      0.69      1200\n",
      "           2       0.73      0.51      0.60      1045\n",
      "\n",
      "    accuracy                           0.62      3283\n",
      "   macro avg       0.69      0.60      0.59      3283\n",
      "weighted avg       0.68      0.62      0.59      3283\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.73      1038\n",
      "           1       0.80      0.79      0.80      1200\n",
      "           2       0.79      0.81      0.80      1045\n",
      "\n",
      "    accuracy                           0.78      3283\n",
      "   macro avg       0.78      0.78      0.78      3283\n",
      "weighted avg       0.78      0.78      0.78      3283\n",
      "\n",
      "XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78      1038\n",
      "           1       0.81      0.86      0.84      1200\n",
      "           2       0.84      0.82      0.83      1045\n",
      "\n",
      "    accuracy                           0.82      3283\n",
      "   macro avg       0.82      0.81      0.82      3283\n",
      "weighted avg       0.82      0.82      0.82      3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use n-grams with CountVectorizer (1 to 3 grams) for crazy feature expansion\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=15000)\n",
    "X_train_ngram = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_ngram = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "# Handle imbalanced data with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_ngram, y_train = smote.fit_resample(X_train_ngram, y_train)\n",
    "\n",
    "# Define multiple models for ensemble learning\n",
    "log_reg = LogisticRegression(max_iter=500, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "nb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_estimators=500, class_weight=\"balanced\", random_state=42)\n",
    "xgb = XGBClassifier(n_estimators=500, random_state=42)\n",
    "\n",
    "# Fit each model\n",
    "log_reg.fit(X_train_ngram, y_train)\n",
    "nb.fit(X_train_ngram, y_train)\n",
    "rf.fit(X_train_ngram, y_train)\n",
    "xgb.fit(X_train_ngram, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log = log_reg.predict(X_test_ngram)\n",
    "y_pred_nb = nb.predict(X_test_ngram)\n",
    "y_pred_rf = rf.predict(X_test_ngram)\n",
    "y_pred_xgb = xgb.predict(X_test_ngram)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"Logistic Regression\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(\"Naive Bayes\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"XGBoost\")\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2GdCLD3egAy",
    "outputId": "0795e764-62ac-4b35-e70c-f11db32e38f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/ProjectMonkeyPox/TraditionalModels/vectorizer.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels', exist_ok=True)\n",
    "\n",
    "# Save models and vectorizer\n",
    "joblib.dump(log_reg, '/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/log_reg.pkl')\n",
    "joblib.dump(nb, '/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/nb.pkl')\n",
    "joblib.dump(rf, '/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/rf.pkl')\n",
    "joblib.dump(xgb, '/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/xgb.pkl')\n",
    "joblib.dump(vectorizer, '/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlHstIUhutXZ"
   },
   "source": [
    "Ensemble Learning ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o0P6EWQxCO4"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd # Import pandas here\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define paths\n",
    "model_path = \"/content/drive/MyDrive/ProjectMonkeyPox/Saved_TraditionalModels/\"\n",
    "\n",
    "# Load models\n",
    "log_reg = joblib.load(model_path + \"log_reg.pkl\")\n",
    "nb = joblib.load(model_path + \"nb.pkl\")\n",
    "rf = joblib.load(model_path + \"rf.pkl\")\n",
    "xgb = joblib.load(model_path + \"xgb.pkl\")\n",
    "vectorizer = joblib.load(model_path + \"vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVUpAkMauwVD",
    "outputId": "bed723b6-644b-4c30-85de-26a95289de16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset again\n",
    "df = pd.read_csv('/content/drive/MyDrive/ProjectMonkeyPox/Monkeypox Dataset.csv')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Ensure preprocessing is applied\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    return text.lower()\n",
    "\n",
    "df[\"Cleaned_Text\"] = df[\"Translated Post Description\"].apply(clean_text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"Processed_Text\"] = df[\"Cleaned_Text\"].apply(preprocess_text)\n",
    "\n",
    "# Sentiment mapping\n",
    "sentiment_mapping = {\n",
    "    \"anger\": \"Negative\",\n",
    "    \"sadness\": \"Negative\",\n",
    "    \"neutral\": \"Neutral\",\n",
    "    \"joy\": \"Positive\"\n",
    "}\n",
    "\n",
    "df[\"Merged_Sentiment\"] = df[\"Sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "# Import LabelEncoder  # This line is added to import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Sentiment_Encoded\"] = label_encoder.fit_transform(df[\"Merged_Sentiment\"])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"Processed_Text\"], df[\"Sentiment_Encoded\"], test_size=0.2, stratify=df[\"Sentiment_Encoded\"], random_state=42\n",
    ")\n",
    "\n",
    "# Transform text using loaded vectorizer\n",
    "X_train_ngram = vectorizer.transform(X_train).toarray()\n",
    "X_test_ngram = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYM_6GzauwSQ",
    "outputId": "76e6a979-30c0-4b28-ea47-59c205e0a17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Voting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75      1038\n",
      "           1       0.75      0.92      0.82      1200\n",
      "           2       0.85      0.79      0.82      1045\n",
      "\n",
      "    accuracy                           0.80      3283\n",
      "   macro avg       0.81      0.79      0.80      3283\n",
      "weighted avg       0.81      0.80      0.80      3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load ensemble learning (Soft Voting) without calling `.fit()`\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', log_reg),\n",
    "        ('nb', nb),\n",
    "        ('rf', rf),\n",
    "        ('xgb', xgb)\n",
    "    ], voting='soft'\n",
    ")\n",
    "\n",
    "\n",
    "y_pred_ensemble = np.mean(\n",
    "    np.stack([model.predict_proba(X_test_ngram) for name, model in ensemble.estimators], axis=0),\n",
    "    axis=0\n",
    ").argmax(axis=1)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "print(\"Ensemble Voting\")\n",
    "print(classification_report(y_test, y_pred_ensemble))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1VVAiwYoHfEyHq85lB_a4KiTuK1xOqHr1",
     "timestamp": 1739862720377
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
