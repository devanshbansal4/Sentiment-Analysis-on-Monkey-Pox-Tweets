{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hHGXYvsETKe",
    "outputId": "a7eb93e5-5d7c-4cc9-fd35-ea3df0e2c651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Post ID                                   Post description        Date  \\\n",
      "0  CgXDOaQDvGm  “I have decided that the global #monkeypox out...  07/23/2022   \n",
      "1  CgXpRmMIdzG  In light of the evolving monkeypox outbreak wi...  07/23/2022   \n",
      "2  CgXaFGDsevq  If you've been hearing about monkeypox and wan...  07/23/2022   \n",
      "3  CgXGNrmLwoL  Monkeypox is a rare disease caused by infectio...  07/23/2022   \n",
      "4  CgXTqcjOQD-  For today's @newyorkermag dispatch. \\n'The Ago...  07/23/2022   \n",
      "\n",
      "  Language                        Translated Post Description Sentiment  \\\n",
      "0  English  “I have decided that the global #monkeypox out...   neutral   \n",
      "1  English  In light of the evolving monkeypox outbreak wi...   neutral   \n",
      "2  English  If you've been hearing about monkeypox and wan...   neutral   \n",
      "3  English  Monkeypox is a rare disease caused by infectio...   neutral   \n",
      "4  English  For today's @newyorkermag dispatch. \\n'The Ago...   sadness   \n",
      "\n",
      "       Hate           Stress or Anxiety  \n",
      "0  Not Hate     Stress/Anxiety Detected  \n",
      "1  Not Hate     Stress/Anxiety Detected  \n",
      "2  Not Hate  No Stress/Anxiety Detected  \n",
      "3  Not Hate  No Stress/Anxiety Detected  \n",
      "4  Not Hate  No Stress/Anxiety Detected  \n",
      "Sentiment\n",
      "neutral    6000\n",
      "joy        5223\n",
      "sadness    3572\n",
      "anger      1617\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn transformers nltk scikit-learn xgboost tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "\n",
    "# Make sure the necessary NLTK data files are downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/ProjectMonkeyPox/Monkeypox Dataset.csv')\n",
    "\n",
    "# Check dataset structure\n",
    "print(df.head())\n",
    "\n",
    "# Original sentiment distribution\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5nyYEG4ETHk"
   },
   "outputs": [],
   "source": [
    "# Clean and preprocess text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n",
    "    text = re.sub(r\"#\", \"\", text)  # Remove hashtags\n",
    "    text = re.sub(r\"[^A-Za-z\\s]\", \"\", text)  # Remove special characters and numbers\n",
    "    return text.lower()\n",
    "\n",
    "df[\"Cleaned_Text\"] = df[\"Translated Post Description\"].apply(clean_text)\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"Processed_Text\"] = df[\"Cleaned_Text\"].apply(preprocess_text)\n",
    "\n",
    "sentiment_mapping = {\n",
    "    \"anger\": \"Negative\",\n",
    "    \"sadness\": \"Negative\",\n",
    "    \"neutral\": \"Neutral\",\n",
    "    \"joy\": \"Positive\"\n",
    "}\n",
    "\n",
    "df[\"Merged_Sentiment\"] = df[\"Sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Sentiment_Encoded\"] = label_encoder.fit_transform(df[\"Merged_Sentiment\"])\n",
    "\n",
    "# Prepare train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"Processed_Text\"], df[\"Sentiment_Encoded\"], test_size=0.2, stratify=df[\"Sentiment_Encoded\"], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVmv-2_lETFQ",
    "outputId": "f8e67e6e-f6e9-4ca0-a936-032c1988ded0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.5570 - loss: 0.8970 - val_accuracy: 0.7810 - val_loss: 0.5318\n",
      "Epoch 2/20\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8614 - loss: 0.3796 - val_accuracy: 0.8099 - val_loss: 0.4892\n",
      "Epoch 3/20\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9509 - loss: 0.1625 - val_accuracy: 0.8145 - val_loss: 0.5648\n",
      "Epoch 4/20\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9754 - loss: 0.0881 - val_accuracy: 0.8203 - val_loss: 0.6537\n",
      "Epoch 5/20\n",
      "\u001b[1m411/411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9903 - loss: 0.0425 - val_accuracy: 0.8185 - val_loss: 0.7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model saved to /content/drive/MyDrive/ProjectMonkeyPox/Saved_CNN/cnn_model.h5\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "CNN Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77      1038\n",
      "           1       0.82      0.85      0.83      1200\n",
      "           2       0.85      0.80      0.82      1045\n",
      "\n",
      "    accuracy                           0.81      3283\n",
      "   macro avg       0.81      0.81      0.81      3283\n",
      "weighted avg       0.81      0.81      0.81      3283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Padding for CNN\n",
    "max_words = 15000  # Vocabulary size\n",
    "max_len = 300     # Max length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# CNN Model\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define model directory on Google Drive\n",
    "model_dir = '/content/drive/MyDrive/ProjectMonkeyPox/Saved_CNN'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "cnn_model_path = os.path.join(model_dir, \"cnn_model.h5\")\n",
    "\n",
    "# Check if model is already saved\n",
    "if os.path.exists(cnn_model_path):\n",
    "    print(\"Loading pre-trained CNN model...\")\n",
    "    model = load_model(cnn_model_path)\n",
    "else:\n",
    "    print(\"Training CNN model...\")\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "        Conv1D(filters=128, kernel_size=5, activation=\"relu\"),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(label_encoder.classes_), activation=\"softmax\")  # Output layer for classification\n",
    "    ])\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train Model\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=20, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(cnn_model_path)\n",
    "    print(f\"CNN model saved to {cnn_model_path}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_cnn = model.predict(X_test_padded)\n",
    "y_pred_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"CNN Classification Report\")\n",
    "print(classification_report(y_test, y_pred_cnn))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1xBsKmDH2hzaaXxIBOBkFd5owPaG0lWa5",
     "timestamp": 1739866165729
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
